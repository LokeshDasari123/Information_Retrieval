{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BIM (binary Independance Model) Implementation"
      ],
      "metadata": {
        "id": "31eJEsmgYFzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have been given a dataset (D1), which contains 1000 documents, distributed into 10 folders.\n",
        "https://www.kaggle.com/datasets/jensenbaxter/10dataset-text-document-classification\n",
        "\n",
        "Task:- Implement BIM model on these 1000 documents.\n",
        "       Skeleton code is given to you. Few method are already implemented.\n",
        "       \n",
        "    \n",
        "Driver code statements are also given in sequence.\n",
        "Run your code after implementation.\n",
        "\n",
        "## Do not clear the outputs."
      ],
      "metadata": {
        "id": "b9Oi2x7Cbuje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QczBAPtWAMOQ",
        "outputId": "8df94126-fac0-4c38-904f-0939765c7a83"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ltQDqHNtXZvM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t22cRnedAv-",
        "outputId": "0329848b-782c-4317-931f-766bf487bedf"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/CSE 419/Lab Assign-4/1000_documents'"
      ],
      "metadata": {
        "id": "6AwMtOBldMVs"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names =os.listdir(dataset_path)\n",
        "len(file_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNrr705mgeeS",
        "outputId": "4a513652-e3a3-434e-a4e0-6c153055dfd4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIR1BmXN71Ww",
        "outputId": "4204ec00-ee11-40a7-dca9-fa7bb7cfca74"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['business_11.txt',\n",
              " 'business_19.txt',\n",
              " 'business_13.txt',\n",
              " 'business_17.txt',\n",
              " 'business_14.txt',\n",
              " 'business_16.txt',\n",
              " 'business_15.txt',\n",
              " 'business_2.txt',\n",
              " 'business_10.txt',\n",
              " 'business_1.txt',\n",
              " 'business_18.txt',\n",
              " 'business_12.txt',\n",
              " 'business_100.txt',\n",
              " 'business_32.txt',\n",
              " 'business_24.txt',\n",
              " 'business_30.txt',\n",
              " 'business_44.txt',\n",
              " 'business_36.txt',\n",
              " 'business_20.txt',\n",
              " 'business_22.txt',\n",
              " 'business_4.txt',\n",
              " 'business_29.txt',\n",
              " 'business_26.txt',\n",
              " 'business_27.txt',\n",
              " 'business_39.txt',\n",
              " 'business_28.txt',\n",
              " 'business_33.txt',\n",
              " 'business_35.txt',\n",
              " 'business_37.txt',\n",
              " 'business_31.txt',\n",
              " 'business_23.txt',\n",
              " 'business_3.txt',\n",
              " 'business_34.txt',\n",
              " 'business_43.txt',\n",
              " 'business_45.txt',\n",
              " 'business_25.txt',\n",
              " 'business_21.txt',\n",
              " 'business_42.txt',\n",
              " 'business_41.txt',\n",
              " 'business_40.txt',\n",
              " 'business_38.txt',\n",
              " 'business_50.txt',\n",
              " 'business_67.txt',\n",
              " 'business_68.txt',\n",
              " 'business_51.txt',\n",
              " 'business_55.txt',\n",
              " 'business_69.txt',\n",
              " 'business_46.txt',\n",
              " 'business_53.txt',\n",
              " 'business_57.txt',\n",
              " 'business_65.txt',\n",
              " 'business_49.txt',\n",
              " 'business_70.txt',\n",
              " 'business_60.txt',\n",
              " 'business_54.txt',\n",
              " 'business_63.txt',\n",
              " 'business_62.txt',\n",
              " 'business_7.txt',\n",
              " 'business_56.txt',\n",
              " 'business_47.txt',\n",
              " 'business_59.txt',\n",
              " 'business_5.txt',\n",
              " 'business_61.txt',\n",
              " 'business_52.txt',\n",
              " 'business_6.txt',\n",
              " 'business_48.txt',\n",
              " 'business_66.txt',\n",
              " 'business_64.txt',\n",
              " 'business_58.txt',\n",
              " 'business_78.txt',\n",
              " 'business_96.txt',\n",
              " 'business_73.txt',\n",
              " 'business_77.txt',\n",
              " 'business_76.txt',\n",
              " 'business_83.txt',\n",
              " 'business_93.txt',\n",
              " 'business_84.txt',\n",
              " 'business_92.txt',\n",
              " 'business_74.txt',\n",
              " 'business_87.txt',\n",
              " 'business_91.txt',\n",
              " 'business_95.txt',\n",
              " 'business_88.txt',\n",
              " 'business_82.txt',\n",
              " 'business_81.txt',\n",
              " 'business_89.txt',\n",
              " 'business_90.txt',\n",
              " 'business_8.txt',\n",
              " 'business_75.txt',\n",
              " 'business_86.txt',\n",
              " 'business_79.txt',\n",
              " 'business_72.txt',\n",
              " 'business_94.txt',\n",
              " 'business_85.txt',\n",
              " 'business_71.txt',\n",
              " 'business_9.txt',\n",
              " 'business_80.txt',\n",
              " 'entertainment_10.txt',\n",
              " 'entertainment_25.txt',\n",
              " 'entertainment_18.txt',\n",
              " 'entertainment_22.txt',\n",
              " 'entertainment_21.txt',\n",
              " 'entertainment_12.txt',\n",
              " 'entertainment_17.txt',\n",
              " 'entertainment_27.txt',\n",
              " 'entertainment_29.txt',\n",
              " 'entertainment_28.txt',\n",
              " 'entertainment_3.txt',\n",
              " 'entertainment_23.txt',\n",
              " 'entertainment_100.txt',\n",
              " 'entertainment_16.txt',\n",
              " 'entertainment_2.txt',\n",
              " 'entertainment_11.txt',\n",
              " 'entertainment_15.txt',\n",
              " 'entertainment_14.txt',\n",
              " 'entertainment_19.txt',\n",
              " 'business_99.txt',\n",
              " 'business_97.txt',\n",
              " 'entertainment_26.txt',\n",
              " 'business_98.txt',\n",
              " 'entertainment_1.txt',\n",
              " 'entertainment_24.txt',\n",
              " 'entertainment_20.txt',\n",
              " 'entertainment_13.txt',\n",
              " 'entertainment_39.txt',\n",
              " 'entertainment_52.txt',\n",
              " 'entertainment_53.txt',\n",
              " 'entertainment_54.txt',\n",
              " 'entertainment_41.txt',\n",
              " 'entertainment_46.txt',\n",
              " 'entertainment_40.txt',\n",
              " 'entertainment_48.txt',\n",
              " 'entertainment_45.txt',\n",
              " 'entertainment_34.txt',\n",
              " 'entertainment_31.txt',\n",
              " 'entertainment_49.txt',\n",
              " 'entertainment_42.txt',\n",
              " 'entertainment_33.txt',\n",
              " 'entertainment_50.txt',\n",
              " 'entertainment_55.txt',\n",
              " 'entertainment_5.txt',\n",
              " 'entertainment_32.txt',\n",
              " 'entertainment_38.txt',\n",
              " 'entertainment_56.txt',\n",
              " 'entertainment_36.txt',\n",
              " 'entertainment_4.txt',\n",
              " 'entertainment_43.txt',\n",
              " 'entertainment_30.txt',\n",
              " 'entertainment_35.txt',\n",
              " 'entertainment_47.txt',\n",
              " 'entertainment_37.txt',\n",
              " 'entertainment_44.txt',\n",
              " 'entertainment_51.txt',\n",
              " 'entertainment_63.txt',\n",
              " 'entertainment_58.txt',\n",
              " 'entertainment_66.txt',\n",
              " 'entertainment_74.txt',\n",
              " 'entertainment_76.txt',\n",
              " 'entertainment_59.txt',\n",
              " 'entertainment_7.txt',\n",
              " 'entertainment_65.txt',\n",
              " 'entertainment_78.txt',\n",
              " 'entertainment_70.txt',\n",
              " 'entertainment_61.txt',\n",
              " 'entertainment_79.txt',\n",
              " 'entertainment_6.txt',\n",
              " 'entertainment_81.txt',\n",
              " 'entertainment_73.txt',\n",
              " 'entertainment_62.txt',\n",
              " 'entertainment_75.txt',\n",
              " 'entertainment_69.txt',\n",
              " 'entertainment_60.txt',\n",
              " 'entertainment_72.txt',\n",
              " 'entertainment_64.txt',\n",
              " 'entertainment_77.txt',\n",
              " 'entertainment_80.txt',\n",
              " 'entertainment_57.txt',\n",
              " 'entertainment_67.txt',\n",
              " 'entertainment_8.txt',\n",
              " 'entertainment_68.txt',\n",
              " 'entertainment_71.txt',\n",
              " 'entertainment_95.txt',\n",
              " 'food_12.txt',\n",
              " 'entertainment_93.txt',\n",
              " 'entertainment_87.txt',\n",
              " 'entertainment_83.txt',\n",
              " 'food_10.txt',\n",
              " 'entertainment_91.txt',\n",
              " 'food_13.txt',\n",
              " 'entertainment_82.txt',\n",
              " 'food_16.txt',\n",
              " 'entertainment_96.txt',\n",
              " 'food_100.txt',\n",
              " 'entertainment_98.txt',\n",
              " 'food_11.txt',\n",
              " 'entertainment_99.txt',\n",
              " 'entertainment_92.txt',\n",
              " 'food_1.txt',\n",
              " 'entertainment_90.txt',\n",
              " 'entertainment_88.txt',\n",
              " 'entertainment_85.txt',\n",
              " 'food_15.txt',\n",
              " 'food_14.txt',\n",
              " 'entertainment_84.txt',\n",
              " 'entertainment_86.txt',\n",
              " 'entertainment_94.txt',\n",
              " 'entertainment_89.txt',\n",
              " 'entertainment_97.txt',\n",
              " 'entertainment_9.txt',\n",
              " 'food_22.txt',\n",
              " 'food_20.txt',\n",
              " 'food_39.txt',\n",
              " 'food_4.txt',\n",
              " 'food_26.txt',\n",
              " 'food_18.txt',\n",
              " 'food_3.txt',\n",
              " 'food_33.txt',\n",
              " 'food_29.txt',\n",
              " 'food_17.txt',\n",
              " 'food_34.txt',\n",
              " 'food_37.txt',\n",
              " 'food_28.txt',\n",
              " 'food_35.txt',\n",
              " 'food_24.txt',\n",
              " 'food_2.txt',\n",
              " 'food_38.txt',\n",
              " 'food_27.txt',\n",
              " 'food_25.txt',\n",
              " 'food_21.txt',\n",
              " 'food_40.txt',\n",
              " 'food_36.txt',\n",
              " 'food_31.txt',\n",
              " 'food_19.txt',\n",
              " 'food_23.txt',\n",
              " 'food_30.txt',\n",
              " 'food_41.txt',\n",
              " 'food_32.txt',\n",
              " 'food_66.txt',\n",
              " 'food_62.txt',\n",
              " 'food_47.txt',\n",
              " 'food_53.txt',\n",
              " 'food_48.txt',\n",
              " 'food_6.txt',\n",
              " 'food_54.txt',\n",
              " 'food_5.txt',\n",
              " 'food_46.txt',\n",
              " 'food_45.txt',\n",
              " 'food_42.txt',\n",
              " 'food_67.txt',\n",
              " 'food_63.txt',\n",
              " 'food_57.txt',\n",
              " 'food_50.txt',\n",
              " 'food_44.txt',\n",
              " 'food_55.txt',\n",
              " 'food_64.txt',\n",
              " 'food_58.txt',\n",
              " 'food_59.txt',\n",
              " 'food_43.txt',\n",
              " 'food_65.txt',\n",
              " 'food_52.txt',\n",
              " 'food_61.txt',\n",
              " 'food_56.txt',\n",
              " 'food_60.txt',\n",
              " 'food_51.txt',\n",
              " 'food_49.txt',\n",
              " 'food_9.txt',\n",
              " 'food_88.txt',\n",
              " 'food_89.txt',\n",
              " 'food_86.txt',\n",
              " 'food_90.txt',\n",
              " 'food_69.txt',\n",
              " 'food_91.txt',\n",
              " 'food_85.txt',\n",
              " 'food_79.txt',\n",
              " 'food_76.txt',\n",
              " 'food_71.txt',\n",
              " 'food_78.txt',\n",
              " 'food_81.txt',\n",
              " 'food_92.txt',\n",
              " 'food_68.txt',\n",
              " 'food_75.txt',\n",
              " 'food_73.txt',\n",
              " 'food_87.txt',\n",
              " 'food_83.txt',\n",
              " 'food_82.txt',\n",
              " 'food_8.txt',\n",
              " 'food_74.txt',\n",
              " 'food_80.txt',\n",
              " 'food_7.txt',\n",
              " 'food_72.txt',\n",
              " 'food_77.txt',\n",
              " 'food_70.txt',\n",
              " 'food_84.txt',\n",
              " 'graphics_10.txt',\n",
              " 'graphics_22.txt',\n",
              " 'graphics_25.txt',\n",
              " 'graphics_2.txt',\n",
              " 'graphics_21.txt',\n",
              " 'graphics_23.txt',\n",
              " 'graphics_18.txt',\n",
              " 'graphics_13.txt',\n",
              " 'graphics_1.txt',\n",
              " 'graphics_12.txt',\n",
              " 'food_97.txt',\n",
              " 'graphics_26.txt',\n",
              " 'food_93.txt',\n",
              " 'graphics_17.txt',\n",
              " 'graphics_11.txt',\n",
              " 'graphics_14.txt',\n",
              " 'food_99.txt',\n",
              " 'food_94.txt',\n",
              " 'food_95.txt',\n",
              " 'graphics_100.txt',\n",
              " 'graphics_19.txt',\n",
              " 'graphics_20.txt',\n",
              " 'graphics_15.txt',\n",
              " 'food_98.txt',\n",
              " 'food_96.txt',\n",
              " 'graphics_16.txt',\n",
              " 'graphics_24.txt',\n",
              " 'graphics_41.txt',\n",
              " 'graphics_32.txt',\n",
              " 'graphics_48.txt',\n",
              " 'graphics_39.txt',\n",
              " 'graphics_44.txt',\n",
              " 'graphics_49.txt',\n",
              " 'graphics_45.txt',\n",
              " 'graphics_27.txt',\n",
              " 'graphics_36.txt',\n",
              " 'graphics_40.txt',\n",
              " 'graphics_38.txt',\n",
              " 'graphics_35.txt',\n",
              " 'graphics_51.txt',\n",
              " 'graphics_50.txt',\n",
              " 'graphics_31.txt',\n",
              " 'graphics_28.txt',\n",
              " 'graphics_5.txt',\n",
              " 'graphics_47.txt',\n",
              " 'graphics_43.txt',\n",
              " 'graphics_34.txt',\n",
              " 'graphics_46.txt',\n",
              " 'graphics_37.txt',\n",
              " 'graphics_30.txt',\n",
              " 'graphics_29.txt',\n",
              " 'graphics_33.txt',\n",
              " 'graphics_4.txt',\n",
              " 'graphics_3.txt',\n",
              " 'graphics_42.txt',\n",
              " 'graphics_70.txt',\n",
              " 'graphics_74.txt',\n",
              " 'graphics_64.txt',\n",
              " 'graphics_65.txt',\n",
              " 'graphics_53.txt',\n",
              " 'graphics_76.txt',\n",
              " 'graphics_63.txt',\n",
              " 'graphics_56.txt',\n",
              " 'graphics_62.txt',\n",
              " 'graphics_61.txt',\n",
              " 'graphics_72.txt',\n",
              " 'graphics_59.txt',\n",
              " 'graphics_75.txt',\n",
              " 'graphics_55.txt',\n",
              " 'graphics_71.txt',\n",
              " 'graphics_68.txt',\n",
              " 'graphics_6.txt',\n",
              " 'graphics_66.txt',\n",
              " 'graphics_60.txt',\n",
              " 'graphics_69.txt',\n",
              " 'graphics_57.txt',\n",
              " 'graphics_77.txt',\n",
              " 'graphics_73.txt',\n",
              " 'graphics_52.txt',\n",
              " 'graphics_58.txt',\n",
              " 'graphics_67.txt',\n",
              " 'graphics_54.txt',\n",
              " 'graphics_7.txt',\n",
              " 'graphics_84.txt',\n",
              " 'graphics_95.txt',\n",
              " 'graphics_98.txt',\n",
              " 'graphics_93.txt',\n",
              " 'graphics_94.txt',\n",
              " 'graphics_89.txt',\n",
              " 'graphics_80.txt',\n",
              " 'historical_100.txt',\n",
              " 'graphics_99.txt',\n",
              " 'graphics_91.txt',\n",
              " 'graphics_90.txt',\n",
              " 'graphics_9.txt',\n",
              " 'graphics_79.txt',\n",
              " 'graphics_85.txt',\n",
              " 'graphics_78.txt',\n",
              " 'graphics_92.txt',\n",
              " 'graphics_97.txt',\n",
              " 'graphics_8.txt',\n",
              " 'historical_1.txt',\n",
              " 'graphics_96.txt',\n",
              " 'graphics_86.txt',\n",
              " 'historical_10.txt',\n",
              " 'graphics_88.txt',\n",
              " 'graphics_87.txt',\n",
              " 'graphics_81.txt',\n",
              " 'graphics_82.txt',\n",
              " 'graphics_83.txt',\n",
              " 'historical_3.txt',\n",
              " 'historical_11.txt',\n",
              " 'historical_20.txt',\n",
              " 'historical_13.txt',\n",
              " 'historical_29.txt',\n",
              " 'historical_24.txt',\n",
              " 'historical_27.txt',\n",
              " 'historical_34.txt',\n",
              " 'historical_32.txt',\n",
              " 'historical_12.txt',\n",
              " 'historical_21.txt',\n",
              " 'historical_14.txt',\n",
              " 'historical_16.txt',\n",
              " 'historical_2.txt',\n",
              " 'historical_15.txt',\n",
              " 'historical_33.txt',\n",
              " 'historical_17.txt',\n",
              " 'historical_26.txt',\n",
              " 'historical_23.txt',\n",
              " 'historical_25.txt',\n",
              " 'historical_28.txt',\n",
              " 'historical_19.txt',\n",
              " 'historical_18.txt',\n",
              " 'historical_35.txt',\n",
              " 'historical_31.txt',\n",
              " 'historical_22.txt',\n",
              " 'historical_30.txt',\n",
              " 'historical_48.txt',\n",
              " 'historical_53.txt',\n",
              " 'historical_38.txt',\n",
              " 'historical_39.txt',\n",
              " 'historical_47.txt',\n",
              " 'historical_57.txt',\n",
              " 'historical_6.txt',\n",
              " 'historical_56.txt',\n",
              " 'historical_59.txt',\n",
              " 'historical_43.txt',\n",
              " 'historical_37.txt',\n",
              " 'historical_58.txt',\n",
              " 'historical_40.txt',\n",
              " 'historical_46.txt',\n",
              " 'historical_4.txt',\n",
              " 'historical_45.txt',\n",
              " 'historical_55.txt',\n",
              " 'historical_42.txt',\n",
              " 'historical_5.txt',\n",
              " 'historical_49.txt',\n",
              " 'historical_51.txt',\n",
              " 'historical_44.txt',\n",
              " 'historical_36.txt',\n",
              " 'historical_41.txt',\n",
              " 'historical_52.txt',\n",
              " 'historical_50.txt',\n",
              " 'historical_54.txt',\n",
              " 'historical_68.txt',\n",
              " 'historical_76.txt',\n",
              " 'historical_64.txt',\n",
              " 'historical_75.txt',\n",
              " 'historical_81.txt',\n",
              " 'historical_66.txt',\n",
              " 'historical_83.txt',\n",
              " 'historical_74.txt',\n",
              " 'historical_82.txt',\n",
              " 'historical_79.txt',\n",
              " 'historical_63.txt',\n",
              " 'historical_84.txt',\n",
              " 'historical_72.txt',\n",
              " 'historical_8.txt',\n",
              " 'historical_65.txt',\n",
              " 'historical_80.txt',\n",
              " 'historical_67.txt',\n",
              " 'historical_60.txt',\n",
              " 'historical_7.txt',\n",
              " 'historical_77.txt',\n",
              " 'historical_78.txt',\n",
              " 'historical_70.txt',\n",
              " 'historical_69.txt',\n",
              " 'historical_62.txt',\n",
              " 'historical_71.txt',\n",
              " 'historical_73.txt',\n",
              " 'historical_61.txt',\n",
              " 'historical_97.txt',\n",
              " 'medical_102.txt',\n",
              " 'historical_86.txt',\n",
              " 'historical_96.txt',\n",
              " 'medical_15.txt',\n",
              " 'medical_103.txt',\n",
              " 'medical_134.txt',\n",
              " 'historical_91.txt',\n",
              " 'historical_90.txt',\n",
              " 'medical_145.txt',\n",
              " 'medical_13.txt',\n",
              " 'medical_152.txt',\n",
              " 'historical_89.txt',\n",
              " 'historical_99.txt',\n",
              " 'historical_88.txt',\n",
              " 'medical_121.txt',\n",
              " 'historical_98.txt',\n",
              " 'historical_95.txt',\n",
              " 'historical_92.txt',\n",
              " 'historical_94.txt',\n",
              " 'historical_93.txt',\n",
              " 'medical_1.txt',\n",
              " 'medical_137.txt',\n",
              " 'historical_87.txt',\n",
              " 'historical_85.txt',\n",
              " 'medical_157.txt',\n",
              " 'historical_9.txt',\n",
              " 'medical_292.txt',\n",
              " 'medical_329.txt',\n",
              " 'medical_300.txt',\n",
              " 'medical_246.txt',\n",
              " 'medical_289.txt',\n",
              " 'medical_295.txt',\n",
              " 'medical_294.txt',\n",
              " 'medical_346.txt',\n",
              " 'medical_349.txt',\n",
              " 'medical_335.txt',\n",
              " 'medical_314.txt',\n",
              " 'medical_248.txt',\n",
              " 'medical_244.txt',\n",
              " 'medical_286.txt',\n",
              " 'medical_322.txt',\n",
              " 'medical_176.txt',\n",
              " 'medical_250.txt',\n",
              " 'medical_318.txt',\n",
              " 'medical_327.txt',\n",
              " 'medical_188.txt',\n",
              " 'medical_278.txt',\n",
              " 'medical_285.txt',\n",
              " 'medical_319.txt',\n",
              " 'medical_186.txt',\n",
              " 'medical_334.txt',\n",
              " 'medical_168.txt',\n",
              " 'medical_158.txt',\n",
              " 'medical_308.txt',\n",
              " 'medical_448.txt',\n",
              " 'medical_41.txt',\n",
              " 'medical_384.txt',\n",
              " 'medical_531.txt',\n",
              " 'medical_500.txt',\n",
              " 'medical_499.txt',\n",
              " 'medical_401.txt',\n",
              " 'medical_458.txt',\n",
              " 'medical_437.txt',\n",
              " 'medical_382.txt',\n",
              " 'medical_364.txt',\n",
              " 'medical_468.txt',\n",
              " 'medical_529.txt',\n",
              " 'medical_449.txt',\n",
              " 'medical_485.txt',\n",
              " 'medical_466.txt',\n",
              " 'medical_534.txt',\n",
              " 'medical_413.txt',\n",
              " 'medical_376.txt',\n",
              " 'medical_438.txt',\n",
              " 'medical_406.txt',\n",
              " 'medical_360.txt',\n",
              " 'medical_385.txt',\n",
              " 'medical_488.txt',\n",
              " 'medical_425.txt',\n",
              " 'medical_487.txt',\n",
              " 'medical_40.txt',\n",
              " 'medical_424.txt',\n",
              " 'medical_631.txt',\n",
              " 'medical_542.txt',\n",
              " 'medical_644.txt',\n",
              " 'medical_564.txt',\n",
              " 'medical_67.txt',\n",
              " 'medical_677.txt',\n",
              " 'medical_565.txt',\n",
              " 'medical_559.txt',\n",
              " 'medical_624.txt',\n",
              " 'medical_579.txt',\n",
              " 'medical_619.txt',\n",
              " 'medical_608.txt',\n",
              " 'medical_560.txt',\n",
              " 'medical_610.txt',\n",
              " 'medical_625.txt',\n",
              " 'medical_660.txt',\n",
              " 'medical_572.txt',\n",
              " 'medical_672.txt',\n",
              " 'medical_646.txt',\n",
              " 'medical_642.txt',\n",
              " 'medical_557.txt',\n",
              " 'medical_590.txt',\n",
              " 'medical_584.txt',\n",
              " 'medical_645.txt',\n",
              " 'medical_58.txt',\n",
              " 'medical_586.txt',\n",
              " 'medical_597.txt',\n",
              " 'medical_60.txt',\n",
              " 'politics_114.txt',\n",
              " 'politics_142.txt',\n",
              " 'medical_694.txt',\n",
              " 'politics_136.txt',\n",
              " 'politics_15.txt',\n",
              " 'politics_130.txt',\n",
              " 'politics_129.txt',\n",
              " 'politics_138.txt',\n",
              " 'politics_149.txt',\n",
              " 'politics_101.txt',\n",
              " 'politics_128.txt',\n",
              " 'politics_112.txt',\n",
              " 'politics_144.txt',\n",
              " 'politics_1.txt',\n",
              " 'politics_139.txt',\n",
              " 'medical_695.txt',\n",
              " 'politics_127.txt',\n",
              " 'medical_692.txt',\n",
              " 'politics_111.txt',\n",
              " 'politics_134.txt',\n",
              " 'politics_137.txt',\n",
              " 'politics_10.txt',\n",
              " 'medical_691.txt',\n",
              " 'politics_117.txt',\n",
              " 'politics_131.txt',\n",
              " 'medical_679.txt',\n",
              " 'politics_119.txt',\n",
              " 'politics_204.txt',\n",
              " 'politics_201.txt',\n",
              " 'politics_2.txt',\n",
              " 'politics_154.txt',\n",
              " 'politics_211.txt',\n",
              " 'politics_212.txt',\n",
              " 'politics_21.txt',\n",
              " 'politics_173.txt',\n",
              " 'politics_155.txt',\n",
              " 'politics_193.txt',\n",
              " 'politics_182.txt',\n",
              " 'politics_172.txt',\n",
              " 'politics_153.txt',\n",
              " 'politics_168.txt',\n",
              " 'politics_192.txt',\n",
              " 'politics_180.txt',\n",
              " 'politics_210.txt',\n",
              " 'politics_189.txt',\n",
              " 'politics_208.txt',\n",
              " 'politics_160.txt',\n",
              " 'politics_20.txt',\n",
              " 'politics_151.txt',\n",
              " 'politics_175.txt',\n",
              " 'politics_19.txt',\n",
              " 'politics_209.txt',\n",
              " 'politics_17.txt',\n",
              " 'politics_188.txt',\n",
              " 'politics_205.txt',\n",
              " 'politics_237.txt',\n",
              " 'politics_229.txt',\n",
              " 'politics_235.txt',\n",
              " 'politics_239.txt',\n",
              " 'politics_226.txt',\n",
              " 'politics_225.txt',\n",
              " 'politics_266.txt',\n",
              " 'politics_277.txt',\n",
              " 'politics_269.txt',\n",
              " 'politics_217.txt',\n",
              " 'politics_221.txt',\n",
              " 'politics_231.txt',\n",
              " 'politics_220.txt',\n",
              " 'politics_222.txt',\n",
              " 'politics_272.txt',\n",
              " 'politics_265.txt',\n",
              " 'politics_247.txt',\n",
              " 'politics_262.txt',\n",
              " 'politics_242.txt',\n",
              " 'politics_236.txt',\n",
              " 'politics_249.txt',\n",
              " 'politics_232.txt',\n",
              " 'politics_275.txt',\n",
              " 'politics_250.txt',\n",
              " 'politics_230.txt',\n",
              " 'politics_271.txt',\n",
              " 'politics_273.txt',\n",
              " 'politics_326.txt',\n",
              " 'space_10.txt',\n",
              " 'politics_317.txt',\n",
              " 'politics_288.txt',\n",
              " 'politics_335.txt',\n",
              " 'politics_301.txt',\n",
              " 'politics_325.txt',\n",
              " 'politics_293.txt',\n",
              " 'politics_322.txt',\n",
              " 'space_1.txt',\n",
              " 'politics_329.txt',\n",
              " 'politics_283.txt',\n",
              " 'politics_31.txt',\n",
              " 'politics_294.txt',\n",
              " 'politics_319.txt',\n",
              " 'politics_298.txt',\n",
              " 'politics_316.txt',\n",
              " 'politics_34.txt',\n",
              " 'politics_321.txt',\n",
              " 'politics_282.txt',\n",
              " 'politics_307.txt',\n",
              " 'space_12.txt',\n",
              " 'politics_287.txt',\n",
              " 'space_100.txt',\n",
              " 'space_11.txt',\n",
              " 'politics_357.txt',\n",
              " 'politics_303.txt',\n",
              " 'politics_313.txt',\n",
              " 'space_37.txt',\n",
              " 'space_25.txt',\n",
              " 'space_20.txt',\n",
              " 'space_34.txt',\n",
              " 'space_28.txt',\n",
              " 'space_19.txt',\n",
              " 'space_24.txt',\n",
              " 'space_26.txt',\n",
              " 'space_18.txt',\n",
              " 'space_14.txt',\n",
              " 'space_33.txt',\n",
              " 'space_35.txt',\n",
              " 'space_32.txt',\n",
              " 'space_23.txt',\n",
              " 'space_17.txt',\n",
              " 'space_38.txt',\n",
              " 'space_16.txt',\n",
              " 'space_21.txt',\n",
              " 'space_22.txt',\n",
              " 'space_29.txt',\n",
              " 'space_30.txt',\n",
              " 'space_36.txt',\n",
              " 'space_13.txt',\n",
              " 'space_27.txt',\n",
              " 'space_15.txt',\n",
              " 'space_3.txt',\n",
              " 'space_2.txt',\n",
              " 'space_31.txt',\n",
              " 'space_4.txt',\n",
              " 'space_40.txt',\n",
              " 'space_39.txt',\n",
              " 'space_53.txt',\n",
              " 'space_57.txt',\n",
              " 'space_6.txt',\n",
              " 'space_59.txt',\n",
              " 'space_54.txt',\n",
              " 'space_45.txt',\n",
              " 'space_51.txt',\n",
              " 'space_55.txt',\n",
              " 'space_47.txt',\n",
              " 'space_58.txt',\n",
              " 'space_42.txt',\n",
              " 'space_60.txt',\n",
              " 'space_44.txt',\n",
              " 'space_50.txt',\n",
              " 'space_56.txt',\n",
              " 'space_49.txt',\n",
              " 'space_63.txt',\n",
              " 'space_52.txt',\n",
              " 'space_62.txt',\n",
              " 'space_41.txt',\n",
              " 'space_5.txt',\n",
              " 'space_48.txt',\n",
              " 'space_46.txt',\n",
              " 'space_61.txt',\n",
              " 'space_43.txt',\n",
              " 'space_65.txt',\n",
              " 'space_68.txt',\n",
              " 'space_69.txt',\n",
              " 'space_86.txt',\n",
              " 'space_83.txt',\n",
              " 'space_84.txt',\n",
              " 'space_64.txt',\n",
              " 'space_8.txt',\n",
              " 'space_89.txt',\n",
              " 'space_87.txt',\n",
              " 'space_71.txt',\n",
              " 'space_74.txt',\n",
              " 'space_73.txt',\n",
              " 'space_80.txt',\n",
              " 'space_82.txt',\n",
              " 'space_79.txt',\n",
              " 'space_70.txt',\n",
              " 'space_88.txt',\n",
              " 'space_67.txt',\n",
              " 'space_81.txt',\n",
              " 'space_72.txt',\n",
              " 'space_7.txt',\n",
              " 'space_85.txt',\n",
              " 'space_75.txt',\n",
              " 'space_77.txt',\n",
              " 'space_78.txt',\n",
              " 'space_66.txt',\n",
              " 'space_76.txt',\n",
              " 'sport_23.txt',\n",
              " 'sport_11.txt',\n",
              " 'sport_18.txt',\n",
              " 'sport_21.txt',\n",
              " 'space_92.txt',\n",
              " 'space_9.txt',\n",
              " 'sport_14.txt',\n",
              " 'space_91.txt',\n",
              " 'space_90.txt',\n",
              " 'sport_22.txt',\n",
              " 'space_99.txt',\n",
              " 'sport_17.txt',\n",
              " 'sport_2.txt',\n",
              " 'space_95.txt',\n",
              " 'sport_20.txt',\n",
              " 'sport_16.txt',\n",
              " 'space_98.txt',\n",
              " 'sport_10.txt',\n",
              " 'sport_100.txt',\n",
              " 'space_97.txt',\n",
              " 'sport_1.txt',\n",
              " 'space_93.txt',\n",
              " 'space_96.txt',\n",
              " 'sport_12.txt',\n",
              " 'sport_13.txt',\n",
              " 'space_94.txt',\n",
              " 'sport_19.txt',\n",
              " 'sport_15.txt',\n",
              " 'sport_31.txt',\n",
              " 'sport_30.txt',\n",
              " 'sport_40.txt',\n",
              " 'sport_48.txt',\n",
              " 'sport_34.txt',\n",
              " 'sport_3.txt',\n",
              " 'sport_39.txt',\n",
              " 'sport_42.txt',\n",
              " 'sport_33.txt',\n",
              " 'sport_35.txt',\n",
              " 'sport_44.txt',\n",
              " 'sport_27.txt',\n",
              " 'sport_28.txt',\n",
              " 'sport_26.txt',\n",
              " 'sport_36.txt',\n",
              " 'sport_38.txt',\n",
              " 'sport_41.txt',\n",
              " 'sport_43.txt',\n",
              " 'sport_47.txt',\n",
              " 'sport_46.txt',\n",
              " 'sport_24.txt',\n",
              " 'sport_4.txt',\n",
              " 'sport_32.txt',\n",
              " 'sport_25.txt',\n",
              " 'sport_45.txt',\n",
              " 'sport_49.txt',\n",
              " 'sport_29.txt',\n",
              " 'sport_37.txt',\n",
              " 'sport_68.txt',\n",
              " 'sport_54.txt',\n",
              " 'sport_63.txt',\n",
              " 'sport_60.txt',\n",
              " 'sport_69.txt',\n",
              " 'sport_64.txt',\n",
              " 'sport_59.txt',\n",
              " 'sport_65.txt',\n",
              " 'sport_58.txt',\n",
              " 'sport_62.txt',\n",
              " 'sport_51.txt',\n",
              " 'sport_7.txt',\n",
              " 'sport_53.txt',\n",
              " 'sport_73.txt',\n",
              " 'sport_56.txt',\n",
              " 'sport_74.txt',\n",
              " 'sport_6.txt',\n",
              " 'sport_57.txt',\n",
              " 'sport_72.txt',\n",
              " 'sport_66.txt',\n",
              " 'sport_70.txt',\n",
              " 'sport_61.txt',\n",
              " 'sport_71.txt',\n",
              " 'sport_67.txt',\n",
              " 'sport_55.txt',\n",
              " 'sport_50.txt',\n",
              " 'sport_52.txt',\n",
              " 'sport_5.txt',\n",
              " 'sport_87.txt',\n",
              " 'sport_88.txt',\n",
              " 'sport_96.txt',\n",
              " 'sport_76.txt',\n",
              " 'sport_90.txt',\n",
              " 'sport_78.txt',\n",
              " 'sport_92.txt',\n",
              " 'sport_82.txt',\n",
              " 'sport_99.txt',\n",
              " 'sport_85.txt',\n",
              " 'sport_89.txt',\n",
              " 'sport_84.txt',\n",
              " 'sport_97.txt',\n",
              " 'sport_91.txt',\n",
              " 'sport_75.txt',\n",
              " 'sport_79.txt',\n",
              " 'sport_9.txt',\n",
              " 'sport_8.txt',\n",
              " 'sport_80.txt',\n",
              " 'sport_95.txt',\n",
              " 'sport_94.txt',\n",
              " 'sport_98.txt',\n",
              " 'sport_81.txt',\n",
              " 'sport_86.txt',\n",
              " 'sport_83.txt',\n",
              " 'sport_93.txt',\n",
              " 'sport_77.txt',\n",
              " 'technologie_32.txt',\n",
              " 'technologie_24.txt',\n",
              " 'technologie_2.txt',\n",
              " 'technologie_1.txt',\n",
              " 'technologie_22.txt',\n",
              " 'technologie_28.txt',\n",
              " 'technologie_13.txt',\n",
              " 'technologie_11.txt',\n",
              " 'technologie_16.txt',\n",
              " 'technologie_100.txt',\n",
              " 'technologie_30.txt',\n",
              " 'technologie_15.txt',\n",
              " 'technologie_17.txt',\n",
              " 'technologie_26.txt',\n",
              " 'technologie_27.txt',\n",
              " 'technologie_12.txt',\n",
              " 'technologie_31.txt',\n",
              " 'technologie_18.txt',\n",
              " 'technologie_3.txt',\n",
              " 'technologie_19.txt',\n",
              " 'technologie_23.txt',\n",
              " 'technologie_33.txt',\n",
              " 'technologie_20.txt',\n",
              " 'technologie_25.txt',\n",
              " 'technologie_29.txt',\n",
              " 'technologie_10.txt',\n",
              " 'technologie_21.txt',\n",
              " 'technologie_14.txt',\n",
              " 'technologie_37.txt',\n",
              " 'technologie_47.txt',\n",
              " 'technologie_56.txt',\n",
              " 'technologie_46.txt',\n",
              " 'technologie_39.txt',\n",
              " 'technologie_35.txt',\n",
              " 'technologie_59.txt',\n",
              " 'technologie_41.txt',\n",
              " 'technologie_52.txt',\n",
              " 'technologie_4.txt',\n",
              " 'technologie_42.txt',\n",
              " 'technologie_49.txt',\n",
              " 'technologie_53.txt',\n",
              " 'technologie_55.txt',\n",
              " 'technologie_58.txt',\n",
              " 'technologie_43.txt',\n",
              " 'technologie_5.txt',\n",
              " 'technologie_54.txt',\n",
              " 'technologie_34.txt',\n",
              " 'technologie_57.txt',\n",
              " 'technologie_50.txt',\n",
              " 'technologie_38.txt',\n",
              " 'technologie_45.txt',\n",
              " 'technologie_48.txt',\n",
              " 'technologie_51.txt',\n",
              " 'technologie_44.txt',\n",
              " 'technologie_40.txt',\n",
              " 'technologie_36.txt',\n",
              " 'technologie_71.txt',\n",
              " 'technologie_72.txt',\n",
              " 'technologie_79.txt',\n",
              " 'technologie_66.txt',\n",
              " 'technologie_60.txt',\n",
              " 'technologie_80.txt',\n",
              " 'technologie_7.txt',\n",
              " 'technologie_63.txt',\n",
              " 'technologie_73.txt',\n",
              " 'technologie_81.txt',\n",
              " 'technologie_76.txt',\n",
              " 'technologie_69.txt',\n",
              " 'technologie_61.txt',\n",
              " 'technologie_74.txt',\n",
              " 'technologie_62.txt',\n",
              " 'technologie_65.txt',\n",
              " 'technologie_78.txt',\n",
              " 'technologie_6.txt',\n",
              " 'technologie_77.txt',\n",
              " 'technologie_64.txt',\n",
              " 'technologie_70.txt',\n",
              " 'technologie_67.txt',\n",
              " 'technologie_83.txt',\n",
              " 'technologie_82.txt',\n",
              " 'technologie_68.txt',\n",
              " 'technologie_75.txt',\n",
              " 'technologie_8.txt',\n",
              " 'technologie_94.txt',\n",
              " 'technologie_98.txt',\n",
              " 'technologie_99.txt',\n",
              " 'technologie_93.txt',\n",
              " 'technologie_92.txt',\n",
              " 'technologie_90.txt',\n",
              " 'technologie_87.txt',\n",
              " 'technologie_97.txt',\n",
              " 'technologie_9.txt',\n",
              " 'technologie_85.txt',\n",
              " 'technologie_95.txt',\n",
              " 'technologie_84.txt',\n",
              " 'technologie_86.txt',\n",
              " 'technologie_96.txt',\n",
              " 'technologie_88.txt',\n",
              " 'technologie_89.txt',\n",
              " 'technologie_91.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G_zLSpfh70nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_dataset():\n",
        "    \"\"\"\n",
        "    This function import all the articles in the TIME corpus,\n",
        "    returning list of lists where each sub-list contains all the\n",
        "    terms present in the document as a string.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    for file in file_names:\n",
        "        with open(os.path.join(dataset_path, file), 'r') as f:\n",
        "            content = f.read().lower()  # read file and convert to lowercase\n",
        "            tokens = word_tokenize(content)  # tokenize words\n",
        "            corpus.append(tokens)\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "mw6cwfpsgs7R"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "f4YaSdqjkA2s"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(corpus):\n",
        "    '''\n",
        "    This function removes stop words from the corpus using NLTK's stopwords.\n",
        "    '''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    clean_corpus = []\n",
        "    for doc in corpus:\n",
        "        clean_doc = [word for word in doc if word not in stop_words and re.match(r'\\w+', word)]\n",
        "        clean_corpus.append(clean_doc)\n",
        "    return clean_corpus"
      ],
      "metadata": {
        "id": "nf-f-6uUkMJ8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inverted_index(corpus):\n",
        "    \"\"\"\n",
        "    This function builds an inverted index as an hash table (dictionary)\n",
        "    where the keys are the terms and the values are ordered lists of\n",
        "    docIDs containing the term.\n",
        "    \"\"\"\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, doc in enumerate(corpus):\n",
        "        for word in set(doc):  # Using 'set' to avoid duplicates\n",
        "            inverted_index[word].append(doc_id)\n",
        "    return inverted_index"
      ],
      "metadata": {
        "id": "jXlXrBxzlJhZ"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def posting_lists_union(pl1, pl2):\n",
        "    \"\"\"\n",
        "    Returns a new posting list resulting from the union of two lists passed as arguments.\n",
        "    \"\"\"\n",
        "    return sorted(set(pl1) | set(pl2))\n"
      ],
      "metadata": {
        "id": "uYWemXG8mXF5"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DF(term, index):\n",
        "    \"\"\"\n",
        "    Compute Document Frequency for a term.\n",
        "    \"\"\"\n",
        "    return len(index[term]) if term in index else 0\n"
      ],
      "metadata": {
        "id": "LL4Rc-L6mmNu"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IDF(term, index, corpus):\n",
        "    '''\n",
        "    Function computing Inverse Document Frequency for a term.\n",
        "    '''\n",
        "    N = len(corpus)\n",
        "    df = DF(term, index)\n",
        "    if df == 0:\n",
        "        return 0\n",
        "    return log((N + 1) / (df + 1))  # Smoothed IDF"
      ],
      "metadata": {
        "id": "FoSxPRDMmwJ0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RSV_weights(corpus,index):\n",
        "    '''\n",
        "    This function precomputes the Retrieval Status Value weights\n",
        "    for each term in the index\n",
        "    '''\n",
        "    N = len(corpus)\n",
        "    w = {}\n",
        "    for term in index.keys():\n",
        "        p = DF(term, index)/(N+0.5)\n",
        "        w[term] = IDF(term, index, corpus) + log(p/(1-p))\n",
        "    return w"
      ],
      "metadata": {
        "id": "jSVqNyyRmzJt"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BIM():\n",
        "    '''\n",
        "    Binary Independence Model class\n",
        "    '''\n",
        "\n",
        "    def __init__(self, corpus):\n",
        "        self.original_corpus = deepcopy(corpus)\n",
        "        self.articles = corpus\n",
        "        self.index = make_inverted_index(self.articles)\n",
        "        self.weights = RSV_weights(self.articles, self.index)\n",
        "        self.ranked = []\n",
        "        self.query_text = ''\n",
        "        self.N_retrieved = 0\n",
        "\n",
        "\n",
        "\n",
        "    def RSV_doc_query(self, doc_id, query):\n",
        "        '''\n",
        "        This function computes the Retrieval Status Value for a given couple:- document - query\n",
        "        using the precomputed weights\n",
        "        '''\n",
        "        score = 0\n",
        "        doc = self.articles[doc_id]\n",
        "        for term in doc:\n",
        "            if term in query:\n",
        "                score += self.weights[term]\n",
        "        return score\n",
        "\n",
        "\n",
        "\n",
        "    def ranking(self, query):\n",
        "        '''\n",
        "        Auxiliary function for the function answer_query. Computes the score only for documents\n",
        "        that are in the posting list of at least one term in the query\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        step 01: find all docs that are in the posting list of at least one term in the query\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        step 02: calculate score for all the docs retrieved in step 01. using RSV_doc_query() function\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        Sort the docs based on the RSV score and return the rankings.\n",
        "        '''\n",
        "        docs_to_score = set()\n",
        "\n",
        "        # Find all docs that are in the posting list of at least one term in the query\n",
        "        for term in query:\n",
        "            if term in self.index:\n",
        "                docs_to_score.update(self.index[term])\n",
        "\n",
        "        # Calculate score for all the docs retrieved\n",
        "        scores = []\n",
        "        for doc_id in docs_to_score:\n",
        "            score = self.RSV_doc_query(doc_id, query)\n",
        "            scores.append((doc_id, score))\n",
        "\n",
        "        # Sort the docs based on the RSV score and return the rankings\n",
        "        ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "        return ranked_docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def answer_query(self, query_text):\n",
        "        '''\n",
        "        Function to answer a free text query. Shows the first 20 words of the\n",
        "        5 most relevant documents.\n",
        "        Also implements the pseudo relevance feedback with k = 5\n",
        "        '''\n",
        "\n",
        "        self.query_text = query_text\n",
        "        query = query_text.lower().split()  # Split query into words\n",
        "        ranking = self.ranking(query)\n",
        "\n",
        "        self.N_retrieved = min(5, len(ranking))  # Handle if fewer than 5 docs are ranked\n",
        "\n",
        "        # Print retrieved documents\n",
        "        for i in range(0, self.N_retrieved):\n",
        "            doc_id = ranking[i][0]\n",
        "            article = self.original_corpus[doc_id]\n",
        "\n",
        "            # Print first 20 words of the article, handling cases where the article has fewer words\n",
        "            first_20_words = ' '.join(article[:20]) if len(article) >= 20 else ' '.join(article)\n",
        "            print(f\"Document {doc_id}: {first_20_words}...\")"
      ],
      "metadata": {
        "id": "5kcgA7fqm7O7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = import_dataset()"
      ],
      "metadata": {
        "id": "qAXKOFw12tJz"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_articles = remove_stop_words(articles)"
      ],
      "metadata": {
        "id": "-DhtmHAo29Kv"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bim = BIM(cleaned_articles)"
      ],
      "metadata": {
        "id": "IvhcrnOm7fjF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 01:-\n",
        "bim.answer_query(\"symptoms of heart attack\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRQF5odo7iXA",
        "outputId": "094b72ff-69b1-441d-eb0f-be64f0fa3528"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 418: war west 1914 german invasion smooth working plan invasion france germans preliminarily reduce ring fortress lige commanded route prescribed 1st...\n",
            "Document 590: cut hicnet medical newsletter page 13 volume 6 number 11 april 25 1993 food drug administration news fda approves depo...\n",
            "Document 442: major developments 1916 western front 1916 1914 centre gravity world war western front 1915 shifted eastern 1916 moved back france...\n",
            "Document 453: battle jutland summer 1916 saw long-deferred confrontation germany high seas fleet great britain grand fleet battle jutlandhistory biggest naval battle...\n",
            "Document 410: eastern fronts 1914 learn war east 1914 eastern front greater distances quite considerable differences equipment quality opposing armies ensured fluidity...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bim.answer_query(\"Impact of war\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY21rjWl8JDG",
        "outputId": "83652c11-d085-4c44-8582-b126a4aff4b3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 459: historiography although opening archives ministry foreign affairs 30-year lock-up enabled new historical research war including jean-charles jauffret book la guerre...\n",
            "Document 486: america first world war jennifer keene explores events led united states america joining first world war describes effect participation war...\n",
            "Document 498: china great war professor xu guoqi provides overview china involvement first world war including role chinese labour corps clc western...\n",
            "Document 504: prisoners war reality prisoners war world war one dr heather jones looks beyond propaganda consider facts around prisoner mistreatment labour...\n",
            "Document 403: world war world war often abbreviated wwi ww1 also known first world war great war global war originating europe lasted...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bim.answer_query(\"3D animations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-izCy5N98aa2",
        "outputId": "8523f003-0b94-4f9d-c6e4-df848c293537"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 298: actually trying write something like encounter problems amongst drawing 3d wireframe view quadric/quartic requires explicit equation quadric/quartic x z functions...\n",
            "Document 299: hi interested writing program generate sird picture know stereogram cross eyes picture becomes 3d anyone one know get one please...\n",
            "Document 308: currently looking 3d graphics library runs ms windows 3.1. libraries visuallib must run vga require add-on graphics cards visuallib run...\n",
            "Document 948: cheaper chip mobiles mobile phone chip combines modem computer processor one bit silicon instead two could make phones cheaper powerful...\n",
            "Document 701: archive-name space/math last-modified date 93/04/01 14:39:12 references frequently recommended net fundamentals astrodynamics roger bate donald mueller jerry white 1971 dover...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bim.answer_query(\"Health benefits\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFS_MGA98lLB",
        "outputId": "cdea43ee-393e-42ae-d0a1-c4e01d34a959"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 564: cut volume 6 number 10 april 20 1993 health info-com network medical newsletter editor david dodell d.m.d 10250 north 92nd...\n",
            "Document 569: cut volume 6 number 11 april 25 1993 health info-com network medical newsletter editor david dodell d.m.d 10250 north 92nd...\n",
            "Document 553: cut limits azt efficacy suggest using drug either sequentially drugs kind aids treatment cocktail combining number drugs fight virus treating...\n",
            "Document 590: cut hicnet medical newsletter page 13 volume 6 number 11 april 25 1993 food drug administration news fda approves depo...\n",
            "Document 562: cut university arizona tucson arizona suggested reading tan sl royston p campbell jacobs hs betts j mason b edwards rg...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bim.answer_query(\"top movies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N321HAh18t6I",
        "outputId": "57078527-c859-4ea0-9351-07eab427d568"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 991: apple laptop gadget' apple powerbook 100 chosen greatest gadget time us magazine mobile pc 1991 laptop chosen one first lightweight...\n",
            "Document 174: de niro film leads us box office film star robert de niro returned top north american box office film hide...\n",
            "Document 195: boogeyman takes box office lead low-budget horror film boogeyman knocked robert de niro thriller hide seek top spot uk box...\n",
            "Document 71: japanese mogul arrested fraud one japan best-known businessmen arrested thursday charges falsifying shareholder information selling shares based false data yoshiaki...\n",
            "Document 100: holmes wins top tv moment' sprinter kelly holmes olympic victory named top television moment 2004 bbc poll holmes 800m gold...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bim.answer_query(\"Key factors for startup success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzQC9uBV80VL",
        "outputId": "f03265f4-c068-47bf-fac9-804aeb123e3f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 714: archive-name space/acronyms edition 8 acronym list sci.astro sci.space sci.space.shuttle edition 8 1992 dec 7 last posted 1992 aug 27 list...\n",
            "Document 491: first world war ended professor david stevenson explains war came end germany accepted harsh terms armistice understand first world war...\n",
            "Document 162: bening makes awards breakthrough film actress annette bening oscar starring role award-winning film julia bening born texas 1958 gained prominence...\n",
            "Document 432: western eastern fronts 1915 western front 1915 repeated french attacks februarymarch 1915 germans trench barrier champagne 500 yards 460 metres...\n",
            "Document 973: wi-fi web reaches farmers peru network community computer centres linked wireless technology providing helping hand poor farmers peru pilot scheme...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TR3N20Va9HgQ"
      },
      "execution_count": 104,
      "outputs": []
    }
  ]
}