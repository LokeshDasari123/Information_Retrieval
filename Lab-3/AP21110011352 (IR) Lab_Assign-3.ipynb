{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oWK5WGeLBxte"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# Path to the folder containing the documents\n",
        "folder_path = '/content/drive/MyDrive/lab3'\n",
        "\n",
        "# Function to read all documents from the folder\n",
        "def load_documents(folder_path):\n",
        "    documents = []\n",
        "    doc_names = os.listdir(folder_path)\n",
        "    for doc_name in doc_names:\n",
        "        doc_path = os.path.join(folder_path, doc_name)\n",
        "        with open(doc_path, 'r', encoding='utf-8') as file:\n",
        "            documents.append(file.read())\n",
        "    return documents, doc_names\n",
        "\n",
        "# Function to tokenize the document\n",
        "def tokenize(text):\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Load and tokenize all documents\n",
        "documents, doc_names = load_documents(folder_path)\n",
        "tokenized_documents = [tokenize(doc) for doc in documents]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inverted_index(tokenized_docs):\n",
        "    index = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    # Build the index\n",
        "    for doc_id, tokens in enumerate(tokenized_docs):\n",
        "        for position, token in enumerate(tokens):\n",
        "            index[token][doc_id].append(position)\n",
        "\n",
        "    return index\n",
        "\n",
        "# Building the index\n",
        "inverted_index = build_inverted_index(tokenized_documents)\n",
        "\n",
        "# Optional: Save the index to a file if needed\n",
        "# import pickle\n",
        "# with pickle.dump(open('inverted_index.pkl', 'wb'), inverted_index)\n",
        "\n",
        "# Output a summary of the index (for debugging)\n",
        "print(f\"Inverted index contains {len(inverted_index)} terms.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blUA07sQHd8d",
        "outputId": "13c82eb0-c327-482a-93c3-a8f67e6f355e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted index contains 8380 terms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_phrasal_query(query, index):\n",
        "    query_terms = tokenize(query)\n",
        "    if not query_terms:\n",
        "        return []\n",
        "\n",
        "    # Get the initial set of documents containing the first term\n",
        "    possible_docs = index[query_terms[0]]\n",
        "\n",
        "    for i, term in enumerate(query_terms[1:], start=1):\n",
        "        next_possible_docs = index[term]\n",
        "\n",
        "        # Check for sequential positions\n",
        "        valid_docs = defaultdict(list)\n",
        "        for doc_id, positions in possible_docs.items():\n",
        "            if doc_id in next_possible_docs:\n",
        "                for pos in positions:\n",
        "                    if pos + 1 in next_possible_docs[doc_id]:\n",
        "                        valid_docs[doc_id].append(pos + 1)\n",
        "\n",
        "        possible_docs = valid_docs\n",
        "\n",
        "    return [doc_names[doc_id] for doc_id in possible_docs.keys()]\n",
        "\n",
        "# Example usage\n",
        "query = \"quick brown fox\"\n",
        "matching_docs = execute_phrasal_query(query, inverted_index)\n",
        "print(f\"Documents matching the query '{query}': {matching_docs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sJZt4chHgdj",
        "outputId": "6a425ad4-2be0-45b6-c5c5-87f4108d4c83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query 'quick brown fox': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Business\"\n",
        "matching_docs = execute_phrasal_query(query, inverted_index)\n",
        "print(f\"Documents matching the query '{query}': {matching_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ4j6PLiHt0g",
        "outputId": "50cfc491-a0cd-4ede-8335-401748e06374"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query 'Business': ['business_93.txt', 'business_99.txt', 'entertainment_89.txt', 'entertainment_93.txt', 'graphics_82.txt', 'historical_8.txt', 'historical_80.txt', 'medical_586.txt', 'politics_271.txt', 'politics_34.txt', 'technologie_93.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"food\"\n",
        "matching_docs = execute_phrasal_query(query, inverted_index)\n",
        "print(f\"Documents matching the query '{query}': {matching_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11PjFDnaIqi8",
        "outputId": "9864cac6-ff26-47eb-9e55-be8a32205359"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query 'food': ['business_93.txt', 'business_91.txt', 'food_84.txt', 'food_90.txt', 'food_86.txt', 'food_88.txt', 'food_87.txt', 'food_83.txt', 'food_85.txt', 'food_89.txt', 'historical_78.txt', 'space_96.txt', 'technologie_88.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" and/or electronics to\"\n",
        "matching_docs = execute_phrasal_query(query, inverted_index)\n",
        "print(f\"Documents matching the query '{query}': {matching_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBNIM2p6IwDA",
        "outputId": "4cf4e333-ac64-43ac-9b0a-4cdcd6f48e6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query ' and/or electronics to': ['technologie_9.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"whether the fixtures\"\n",
        "matching_docs = execute_phrasal_query(query, inverted_index)\n",
        "print(f\"Documents matching the query '{query}': {matching_docs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5aWYYTlI9ls",
        "outputId": "55265530-7080-451a-eb4e-cf6d772a06b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents matching the query ' whether the fixtures': ['sport_99.txt']\n"
          ]
        }
      ]
    }
  ]
}